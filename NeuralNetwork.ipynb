{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NeuralNetwork.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOCPO1SZS+WCUelF2ERNbpj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Raharijao/CMP-414/blob/main/NeuralNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#Takes in an array, changes all values that\n",
        "# are less than 0 to a 0, and then returns the result\n",
        "def Relu(x):\n",
        "    x = np.where(x < 0, 0, x)\n",
        "    return x\n",
        "\n",
        "#Takes in an array, changes all values greater than\n",
        "# 0 into 1, and anything else into a 0\n",
        "def Relu_derivative(i):\n",
        "    i = np.where(i > 0, 1, i)\n",
        "    i = np.where(i <= 0, 0, i)\n",
        "    return i \n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, x, y):\n",
        "        self.input      = x #Array of inputs\n",
        "\n",
        "        #array of the weights between input layer and hidden layer.\n",
        "        # 4 represents the number of nodes (not directly, but it represents the\n",
        "        # number of columns, and each columns represents a node)\n",
        "        # \"self.input.shape[1]\" represents the number of rows, each row\n",
        "        # represents one of the input nodes, and each value in a row represents\n",
        "        # it's weight value with each of the 4 corresponding hidden layer nodes. \n",
        "        \n",
        "        self.weights1   = np.random.rand(self.input.shape[1],4)\n",
        "        self.biase1     = np.random.rand(self.weights1.shape[1],1) \n",
        "\n",
        "        #Same thing, but for a second hidden layer\n",
        "        self.weights2   = np.random.rand(self.weights1.shape[1],4)\n",
        "        self.biase2     = np.random.rand(self.weights2.shape[1],1) \n",
        "\n",
        "        #array of the weights between last hidden layer and output layer\n",
        "        self.weights3   = np.random.rand(4,1)\n",
        "\n",
        "        #Array of expected outputs (according to the inputs, ex 5 set of input\n",
        "        # values will give 5 output values)\n",
        "        self.y          = y \n",
        "\n",
        "        #Makes an array of size (y.shape) filled with 0, this will be used to\n",
        "        # store the predicted outputs\n",
        "        self.output     = np.zeros(y.shape) \n",
        "\n",
        "\n",
        "    #Calculating the prediction based on the current weights and biases in each\n",
        "    # layer with the activation function\n",
        "    def predict(self):\n",
        "        #ReLu( inputs * weights + biase)\n",
        "        self.layer1 = Relu(np.dot(self.input, self.weights1) + self.biase1) \n",
        "        self.layer2 = Relu(np.dot(self.layer1, self.weights2) + self.biase2) \n",
        "        self.output = Relu(np.dot(self.layer2, self.weights3))\n",
        "        \n",
        "\n",
        "    # here I am trying to imporve the performance of my model\n",
        "    # To do so I use the MSE as a loss function to see the current performance\n",
        "    # I need to find the weights and biase values that would make it so that the\n",
        "    # loss value is at it's lowest\n",
        "    # To do so, I am going to apply gradient descent on the graph of the loss\n",
        "    # function, But for doing that, I need the derivative of the loss function, \n",
        "    # in respect to the weights and biases.\n",
        "    # To get this, I must apply the chain rule.\n",
        "    # Loss function used is the Sum of Squared errors\n",
        "    def fit(self):\n",
        "        \n",
        "        d_weights3 = np.dot(self.layer2.T,(2*(self.y - self.output) \n",
        "                            * Relu_derivative(self.output)))\n",
        "        d_weights2 = np.dot(self.layer1.T,(np.dot(2*(self.y - self.output) \n",
        "                            * Relu_derivative(self.output), self.weights3.T) \n",
        "                            * Relu_derivative(self.layer2)))\n",
        "        d_weights1 = np.dot(self.input.T,  (np.dot(2*(self.y - self.output)\n",
        "                            * Relu_derivative(self.output), self.weights3.T) \n",
        "                            * Relu_derivative(self.layer2) * self.weights2.T \n",
        "                            * Relu_derivative(self.layer1)))\n",
        "        \n",
        "        # update the weights with the derivative (slope) of the loss function\n",
        "        self.weights1 += d_weights1\n",
        "        self.weights2 += d_weights2\n",
        "        self.weights3 += d_weights3\n",
        "        \n",
        "\n",
        "import tensorflow as tf # imported to get dataset\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    #Test data set\n",
        "    X = np.array([[7,0,1,0,1,0],\n",
        "                  [0,2,1,0,1,0],\n",
        "                  [1,0,1,5,1,0],\n",
        "                  [1,6,1,0,1,0]])\n",
        "    y = np.array([[10],[1],[8],[0]])\n",
        "    nn = NeuralNetwork(X,y)\n",
        "\n",
        "    #Trains the model multiple times\n",
        "    for i in range(15):\n",
        "        nn.predict()\n",
        "        nn.fit()\n",
        "        \n",
        "    #Print out the model's predictions after training\n",
        "    #For some reasons that I am still not sure of, the model only outputs 0s\n",
        "    print(nn.output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWnHI7n9AzI_",
        "outputId": "9f787eae-b977-4552-a6ea-dacd45f6144b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.]\n",
            " [0.]\n",
            " [0.]\n",
            " [0.]]\n"
          ]
        }
      ]
    }
  ]
}